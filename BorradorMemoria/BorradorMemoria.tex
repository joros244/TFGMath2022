\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage[spanish,es-noshorthands]{babel}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{amsfonts}
\usepackage{authblk}
\usepackage{mathtools}

\usepackage{graphicx}
\graphicspath{{Images/}}
\usepackage{float}
\usepackage{fullpage}
\parskip = 2pt plus 1.5pt minus 1pt
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{babel}
\pgfdeclarelayer{background}
\pgfdeclarelayer{foreground}
\pgfsetlayers{background,main,foreground}

\newcommand\hlightr[1]{\tikz[overlay, remember picture,baseline=-
\the\dimexpr\fontdimen22\textfont2\relax]\node[rectangle,fill=red!50,rounded 
corners,fill opacity = 0.2,draw,thick,text opacity =1] {$#1$};}

\numberwithin{equation}{section}

\newtheorem{theorem}{Teorema}
\theoremstyle{definition}
\newtheorem{defi}{Definición}
\newtheorem{ejemplo}{Ejemplo}
\newenvironment{ejem}
  {\pushQED{\qed}\renewcommand{\qedsymbol}{$\blacktriangleleft$}\ejemplo}
  {\popQED\endejemplo}

\theoremstyle{remark}
\newtheorem*{remark}{Nota}

\theoremstyle{plain}
\newtheorem{prop}{Proposición}

\title{Sobre la homología persistente en redes neuronales}


\author{José Manuel Ros Rodrigo}

\affil{Facultad de Ciencia y Tecnología\\
  Universidad de La Rioja}

\date{Mayo 2022}

\begin{document}
	
	\maketitle
	
	\newpage
	
	\begin{abstract}
	(En construcción.)
	\end{abstract}
	
	\newpage

	\tableofcontents

	\newpage

	\section{Introducción}
	(En construcción.)

	\newpage

	\section{Preeliminares}

	A lo largo de este capítulo vamos a ver todas las nociones teóricas 
	necesarias para el uso de la homología persistente en redes neuronales.	
	
	\subsection{Complejos simpliciales}
	Comenzamos con el primer concepto fundamental de todo el trabajo, los 
	\emph{complejos simpliciales}. Esta noción admite dos enfoques 
	diferentes, por lo que debemos dintinguir entre dos definiciones 
	relacionadas: los complejos simpliciales \emph{abstractos} y los 
	complejos simpliciales \emph{geométricos}. Para el desarrrollo estas 
	nociones seguiremos la guia proporcionada por \cite{}.

	Siguiendo el enfoque combinatorio, comenzamos definiendo los complejos 
	simpliciales abstractos y algunas nociones relacionadas. 

	\begin{defi}
	
	Un \textit{complejo simplicial abstracto} es una colección finita, 
	\begin{Large}$\nu$\end{Large}, de subconjuntos no vacíos de un 
	conjunto, {\Large $\nu$}$_{0}$, que verifica las siguientes 
	propiedades:
	
	\begin{enumerate}
		\item Si $v \in $ {\Large $ \nu$}$_{0}$, entonces $\{v\} \in$
			\begin{Large}$ \nu$\end{Large}
		\item Si $\sigma \in $ {\Large $ \nu$}$ \text{ y } \tau 
			\subset \sigma$, entonces $ \tau \in $
			\begin{Large}$ \nu$\end{Large}
	\end{enumerate}
	
	A los elementos de {\Large $\nu$} los llamaremos \textit{símplices};
	más concretamente: dado $\sigma \in $ {\Large $\nu$}, diremos que 
	$\sigma$ tiene \textit{dimensión p}, y que $\sigma$ es un 
	\textit{p-símplice}, si $|\sigma|=p+1$. Asimismo, definimos la 
	\textit{dimensión de {\Large $\nu$}} como el máximo de las dimensiones 
	de sus símplices y denotaremos por {\Large $\nu$}$_{p}$ a la colección 
	de los p-símplices de {\Large $\nu$}.	
	
	\end{defi}
	
	En relación con el concepto de símplice y de dimensión surge la 
	siguiente noción: 

	\begin{defi}
		Sean $\sigma$ y $\tau$ dos símplices de {\Large $\nu$} tales 
		que $\tau \subset \sigma$. Entonces diremos que $\tau$ es una 
		\textit{cara} de $\sigma$, y además, si las dimensiones de 
		$\sigma$ y $\tau$ difieren por un número natural $a$, 
		diremos que $\tau$ es una cara de $\sigma$ de 
		\textit{codimensión a}.
	\end{defi}

	Ahora que hemos definido los complejos simpliciales abstractos veamos 
	un pequeño ejemplo para fijar ideas.

	\begin{ejem}
		\label{ejem:1}
		Supongamos el siguiente complejo simplicial abstracto:
		\begin{multline*} 
			\text{{\Large $\nu$}}=\{\{a\},\{b\},\{c\},\{d\},
			\{a,b\},\{a,c\},\{a,d\},\{b,c\},\{b,d\},\{c,d\},
			\{a,b,c\},\{a,b,d\},\\
			\{a,c,d\},\{a,b,c,d\}\}
		\end{multline*}
		Así, tenemos que la dimensión de {\Large $\nu$} es 3. También 
		observamos que el 3-símplice $\{a,b,c,d\}$ tiene por caras de 
		codimensión 1 a los 2-símplices $\{a,b,c\},\{a,b,d\}$ y 
		$\{a,c,d\}$. En la figura \ref{fig:tetra} ilustramos una 
		representación geométrica de {\Large $\nu$}.

		\begin{figure}[H]
			\label{fig:tetra}
			\centering
			\begin{tikzpicture}
				%Nodes
				\coordinate      (a) 	at 	(4.5,2.5);
				\coordinate      (b) 	at 	(3,0.8);
				\coordinate     (c) 	at 	(4.4,0.1);
				\coordinate      (d) 	at 	(6,0.8);
				
				%Lines
		    		\draw[thick, fill=black!20] (a) -- (b) -- (c) -- (d) -- cycle;
				\draw[thick, dashed] (b) -- (d);
				\draw[thick] (a) -- (c);

				\fill[black!20, draw=black, thick] (a) circle (3pt) node[black, above right] {a};
				\fill[black!20, draw=black, thick] (b) circle (3pt) node[black, above left] {b};
				\fill[black!20, draw=black, thick] (c) circle (3pt) node[black, below right] {c};
				\fill[black!20, draw=black, thick] (d) circle (3pt) node[black, above right] {d};

			\end{tikzpicture}
			\caption{Representación geométrica del complejo 
			simplicial {\Large $\nu$}.}
		\end{figure}

		La representación recogida en la figura \ref{fig:tetra}, en la
		que cada símplice corresponde con un poliedro regular (cada 
		$0$-símplice corresponde a un punto, cada $1$-símplice a una 
		arista, cada $2$-símplice a un triángulo, cada $3$-símplice a 
		un tetraedro, etc.) es única salvo homeomorfismo. Observamos 
		que interpretando {\Large $\nu$} como un subconjunto de 
		$\mathbb{R}^{3}$ obtenemos un tetraedro. Esta idea motiva el 
		otro enfoque de los complejos simpliciales: el enfoque 
		geométrico.
	\end{ejem}

	Siguiendo el enfoque geométrico es necesario que, antes de llegar a la
	definición de complejo simplicial geométrico, veamos unos conceptos 
	previos relacionados con la propia definición.

	\begin{defi}
		Sean $\{u_{0},u_{1},...,u_{k}\}\subset\mathbb{R}^{n}$. Diremos
		que los k+1 puntos son \textit{afínmente independientes} si 
		los k vectores $u_{1}-u_{0},u_{2}-u_{0},...,u_{k}-u_{0}$ son
		linealmente independientes.

		Sea $x \in \mathbb{R}^{n}$. Diremos que $x$ es una 
		\textit{combinación afín} de $\{u_{0},u_{1},...,u_{k}\}$ si 
		$\exists \lambda_{0},...,\lambda_{k}$ tales que 
		$x=\sum_{i=0}^{k}\lambda_{i}u_{i}$ y 
		$\sum_{i=0}^{k}\lambda_{i}=1$
	\end{defi}

	\begin{defi}
		Sean $\{u_{0},u_{1},...,u_{k}\}\subset\mathbb{R}^{n}$ k+1 
		puntos afínmente independientes y $x=\sum_{i=0}^{k}
		\lambda_{i}u_{i}$ una combinación afín. Diremos que $x$ es una
		\textit{combinación convexa} de $\{u_{0},u_{1},...,\\
		u_{k}\}$ si $\{\lambda_{0},\lambda_{1},...,\lambda_{k}\}$ son 
		no negativos.

		Definimos la \textit{clausura convexa} de $\{u_{0},u_{1},...,
		u_{k}\}$ como el conjunto de todas sus posibles combinaciones 
		convexas.
	\end{defi}

	Ahora que ya contamos con estas nociones previas pasamos a definir la 
	pieza clave en la definición de complejo simplicial geométrico: el 
	\emph{símplice}.

	\begin{defi}
		Definimos un \textit{k-símplice} como la clausura convexa de 
		k+1 puntos afínmente independientes. Lo denotaremos por 
		$\sigma=conv\{u_{0},u_{1},...,u_{k}\}$, y diremos que la 
		\textit{dimensión de $\sigma$} es k.

		Llamamos \textit{cara} de $\sigma$ a cualquier combinación 
		convexa de un subconjunto no vacío de $\{u_{0},u_{1},...,
		u_{k}\}$. A la relación <<ser cara de>> la denotaremos por 
		$\leq$.

		Para los casos $k=0,1,2,3$ diremos que $\sigma$ es un vértice,
		arista, triángulo, tetraedro respectivamente.
	\end{defi}

	Habiendo definido todos los conceptos previos necesarios pasamos a 
	definir \emph{complejo simplicial geométrico}.

	\begin{defi}
		Llamamos \textit{complejo simplicial geométrico} a la 
		colección finita de símplices {\Large $\nu$} verificando las
		siguientes propiedades:
		\begin{enumerate}
			\item Si $\sigma\in \text{{\Large $\nu$} y }
				\tau \leq \sigma \implies \tau \in 
				\text{{\Large $\nu$}}$
			\item Si $\sigma_{1},\sigma_{2} \in 
				\text{{\Large $\nu$}} \implies 
				\sigma_{1}\cap\sigma_{2}=\emptyset \text{ o }
				\sigma_{1}\cap\sigma_{2} \text{ es una cara 
				común a ambos.}$
		\end{enumerate}
	\end{defi}
	
	La relación entre los complejos simpliciales abstractos y los 
	geométricos viene dada por la construcción de la \emph{realización
	geométrica} de un complejo simplicial abstracto, que es un complejo 
	simplicial geométrico definido tal y como se ilustra en la figura 
	\ref{fig:tetra} del ejemplo \ref{ejem:1}(para más detalles véase 
	\cite{}).

	De aquí en adelante emplearemos la definición de complejo simplicial 
	abstracto, pues es la más adecuada para el presente trabajo.

	Ahora que ya hemos definido los objetos con los que vamos a trabajar, 
	procedemos a definir las aplicaciones entre ellos.
	\begin{defi}
		Una \textit{aplicación simplicial entre complejos 
		simpliciales}, $f:\text{{\Large $\nu$}} \rightarrow 
		\text{{\Large $\nu$}$^{\prime}$}$, es una aplicación 
		tal que $f(\sigma)=\{g(u_{1}),g(u_{2}),...,g(u_{k})\}=
		\{v_{1},v_{2},...,v_{k}\}$; donde
		$g:\text{{\Large $\nu$}$_{0}$} \rightarrow \text{{\Large 
		$\nu$}$_{0}^{\prime}$}$ es una aplicación entre $0$-símplices,
		$\sigma=\{u_{1},u_{2},...,u_{k}\} \in \text{{\Large $\nu$}}$
		y $\{v_{1},v_{2},...,v_{k}\} \in \text{{\Large $\nu$}$^{
		\prime}$}$.	
	\end{defi}

	\newpage
	
	\subsection{Homología. Homología persistente}
	
	En la sección anterior hemos fijado el concepto de complejo 
	simplicial, que nos será muy útil a lo largo de esta sección para 
	desarrollar la noción de \emph{espacio vectorial de homología}. A 
	diferencia de como surgió el concepto de \emph{espacio vectorial de
	homología} en la historia de las matemáticas, en el presente trabajo 
	desarrollaremos primero la noción general para luego reducir al caso 
	particular de la \emph{homología simplicial}. Para ello emplearemos la 
	guía proporcionada por \cite{}.

	Comenzamos la sección con una definición básica, que no aparece en el 
	grado, y que nos será necesaria para nuestros propósitos.

	\begin{defi}
		Sea $R$ anillo. Definimos el \textit{R-módulo izquierdo} sobre
		$R$ como el conjunto $M$ junto con las operaciones:
		\begin{itemize}
			\item Suma: $M \times M \rightarrow M, (x,y) \mapsto 
				x+y$, y
			\item Producto por escalares: $R \times M \rightarrow 
				M, (r,x) \mapsto rx$,	
		\end{itemize}
		satisfaciendo las siguientes propiedades:
		\begin{enumerate}
			\item La suma es asociativa, conmutativa, $M$ contiene
			      un elemento neutro para ella y todo elemento 
			      tiene opuesto. Es decir, $(M,+)$ es un grupo 
			      abeliano.
		      \item Para cualesquiera $x,y$ de $M$ y $r,s$ de $R$: 
		      	\begin{enumerate}
			 	\item $(r+s)x=rx+sx$ (distributiva respecto a 
					la suma de $R$).
				\item $(rs)x=r(sx)$ (asociativa).
				\item $r(x+y)=rx+ry$ (distributiva respecto a 
					la suma de $M$).
				\item Si $R$ es unitario, $1x=x$.	
			\end{enumerate}
		\end{enumerate}
		De manera análoga definimos el \textit{R-módulo derecho}. Si 
		$R$ es conmutativo, entonces el $R$-módulo izquierdo es el 
		mismo que el $R$-módulo derecho. En tal caso nos referiremos a
		él simplemente como \textit{R-módulo}.
	\end{defi}

	\begin{remark}
	De la noción de $R$-módulo nos interesan particularmente las 
	siguientes propiedades: todo grupo abeliano es $\mathbb{Z}$-módulo, y
	si $R$ es un cuerpo, entonces las nociones de $R$-módulo y $R$-espacio
	vectorial coinciden.
	\end{remark}

	Como es natural, a la noción de $R$-módulo le sigue la definición de
	\emph{R-submódulo}.

	\begin{defi}
	Sea $M$ un $R$-módulo. Definimos el \textit{R-submódulo} de $M$ como 
	el subconjunto, no vacío, $N$ de $M$ tal que es cerrado para opuestos 
	y para las operaciones heredadas de $M$. A la relación <<ser submódulo 
	de>> la denotaremos por $\leq$.
	\end{defi}

	Tras estas consideraciones básicas, comenzamos el camino que nos 
	conducirá a la definición de los \emph{espacios vectoriales de 
	homología}. Empezamos el camino con la definición de \emph{complejo de 
	cadenas}.

	\begin{defi}
	Sea $R$ anillo. Decimos que un \textit{complejo de cadenas} sobre $R$ 
	es un conjunto $\mathcal{C}_{*}=\{(C_{p},\partial_{p}) | p \in \mathbb{Z}\}$
	de $R$-módulos y $R$-homomorfismos $\{\partial_{p}: C_{p} \rightarrow 
	C_{p-1} | p \in \mathbb{Z}\}$, satisfaciendo $\partial_{p}\circ 
	\partial_{p+1}=0$. Se denota por $(\mathcal{C}_{*},\partial)$ y a 
	$\partial$ se le llama el diferencial del complejo.
	\end{defi}

	Notemos que la propiedad anterior es equivalente a que 
	$Im(\partial_{p+1}) \leq Ker(\partial_{p}), p \in \mathbb{Z}$. Es 
	habitual pensar en $\mathcal{C}_{*}$ como una sucesión infinita cuya
	representación es como sigue:

	\begin{equation*}
		 \left.
		\begin{array}{ccccccccc}
			 & \partial_{2} &  & \partial_{1} &  & 
			\partial_{0} &  & \partial_{-1} & \\ 
			\cdots & \longrightarrow & C_{1}& 
			\longrightarrow & C_{0} 
					& \longrightarrow 
			& C_{-1} & 
			\longrightarrow & \cdots 
		\end{array}
		\right. 
	\end{equation*}

	Fijemos ahora nuestra atención en los $R$-submódulos de $C_{p}$: 
	$Im(\partial_{p+1})$ y $Ker(\partial_{p})$.

	\begin{defi}
		Sea $(\mathcal{C}_{*},\partial)$ un complejo de cadenas sobre 
		$R$ anillo. 
		Definimos los siguientes $R$-submódulos de $C_{p}$:
		\begin{enumerate}
			\item $Z_{p}(\mathcal{C}_{*})=Ker(\partial_{p})$. A 
				sus elementos los llamaremos $p$-ciclos.
			\item $B_{p}(\mathcal{C}_{*})=Im(\partial_{p+1})$. A 
				sus elementos los llamaremos $p$-bordes.
		\end{enumerate}
	\end{defi}

	Ahora bien, en virtud de la relación de inclusión entre ambos 
	submódulos y de las operaciones heredadas, podemos considerar el 
	\emph{$R$-módulo cociente} $Z_{p}(\mathcal{C}_{*})/B_{p}(
	\mathcal{C}_{*})$. Este cociente constituye una pieza fundamental del 
	presente trabajo y merece una definición detallada:

	\begin{defi}
		Sea $(\mathcal{C}_{*},\partial)$ un complejo de cadenas sobre 
		$R$ anillo. Definimos el $p$-ésimo $R$-módulo de homología de
		$\mathcal{C}_{*}$ como el cociente:
		$$H_{p}(\mathcal{C}_{*}):=Z_{p}(\mathcal{C}_{*})/B_{p}(
		\mathcal{C}_{*})$$
	\end{defi}

	Ahora que ya tenemos una noción general de los $R$-módulos de 
	homología, pasamos al caso particular que nos ocupa en el presente 
	trabajo: \emph{la homología simplicial}. El primer paso será definir 
	el complejo de cadenas asociado a un complejo simplicial, para ello 
	será necesario fijar los $R$-módulos y $R$-homomorfismos apropiados.

	Ante esta decisión, lo habitual es fijar una orientación para los 
	símplices (la dada por el orden lexicográfico de sus vértices) y 
	definir los grupos abelianos de cadenas dados por las sumas formales 
	de los símplices con coeficientes en $\mathbb{Z}$ 
	($\mathbb{Z}$-módulos). 

	No obstante, en el presente trabajo ignoramos 
	la noción de orientación, pues no aporta beneficio alguno a nuestros 
	propósitos, es más, podría perjudicarlos. Así pues, a partir de ahora,
	fijaremos el anillo $R=\mathbb{Z}_{2}$, con lo que podremos ver los 
	$\mathbb{Z}_{2}$-módulos que conformarán nuestro complejo de cadenas 
	como $\mathbb{Z}_{2}$-espacios vectoriales cuya base vendrá dada por 
	los símplices del complejo simplicial escogido.

	Veamos esto de manera más formal:

	\begin{defi}
		Sea {\Large $\nu$} un complejo simplicial y $p \in \mathbb{N}
		\cup\{0\}$ 
		tal que $p\leq dim \text{{\Large $\nu$}}$. Una 
		\textit{p-cadena} es una suma formal de p-símplices de 
		{\Large $\nu$}. Es decir, si $c$ es una p-cadena, entonces 
		$c=\sum a_{i}\sigma_{i},\sigma_{i} \in \text{{\Large $\nu$}},
		a_{i} \in \mathbb{Z}_{2}$. 
	\end{defi}
	
	Con la noción de p-cadena, pasamos a la definición de los 
	$\mathbb{Z}_{2}$-módulos que hemos introducido anteriormente. En 
	adelante los denotaremos por $C_{p}(\text{{\Large $\nu$}})$. 

	\begin{defi}
		Sea {\Large $\nu$} un complejo simplicial abstracto. Definimos
		el \textit{$\mathbb{Z}_{2}$-módulo de p-cadenas} de 
		{\Large $\nu$} como el conjunto de todas las p-cadenas de 
		{\Large $\nu$}, con la operación suma componente a componente 
		con coeficientes en $\mathbb{Z}_{2}$, y el producto externo 
		por elementos de $\mathbb{Z}_{2}$ usual. Lo denotaremos por 
		$(C_{p}(\text{{\Large $\nu$}}), +)$ o simplemente 
		$C_{p}(\text{{\Large $\nu$}})$. 	
	\end{defi}
	
	Tal y como ya hemos advertido antes, estos $\mathbb{Z}_{2}$-módulos 
	pueden ser vistos como espacios vectoriales cuyas bases vienen dadas 
	por los p-símplices de {\Large $\nu$}. Así pues, en adelante, por 
	simplicidad del lenguaje nos referiremos a ellos como \emph{espacios 
	vectoriales de p-cadenas}.

	Habiendo especificado los $R$-módulos que vamos a emplear, pasamos a
	definir los $R$-homomorfismos asociados.

	\begin{defi}
		Sea {\Large $\nu$} un complejo simplicial abstracto, $\sigma 
		\in \text{{\Large $\nu$}}$ y $\sigma = \{u_{0},...,u_{p}\}$. 
		Definimos el \textit{operador borde para un símplice}
		como:
		$$
		B_{p}(\sigma)=\displaystyle 
		\sum_{j=0}^{p}\{u_{0},...,\widehat{u_{j}},...,u_{p}\}
		$$
 		La suma anterior es una suma formal, donde $\widehat{u_{j}}$ 
		indica que omitimos $u_{j}$. Ahora extendemos dicho operador 
		para cadenas, en concreto:
	\begin{equation}
		\label{def:borde_form}
		\begin{array}{lll}
			\partial_{p}:C_{p}(\text{{\Large $\nu$}}) & 
				\rightarrow & C_{p-1}(\text{{\Large $\nu$}})
				\\[3pt] 
			\multicolumn{1}{r}{c=\sum\sigma_{i}} & \mapsto & 
			\partial_{p}(c)=\sum B_{p}(\sigma_{i})
		\end{array}
	\end{equation}
	En esta definición hemos omitido los coeficientes en las cadenas pues
	trabajamos sobre $\mathbb{Z}_{2}$. A $\partial_{p}$ lo llamaremos 
	\textit{homomorfismo borde}.	
	\end{defi}

	\begin{proof}
		Vamos a probar que $\partial_{p}$ es, en efecto, un 
		$\mathbb{Z}_{2}$-homomorfismo.

		Sean $\sigma,\tau \in \text{{\Large $\nu$}}_{p}$ tales que
		$\sigma = \{u_{0},...,u_{p}\}$ y $\tau = \{w_{0},...,w_{p}\}$.

		$B_{p}(\sigma)+B_{p}(\tau)=\displaystyle 
		\sum_{j=0}^{p}\{u_{0},...,\widehat{u_{j}},...,u_{p}\} + 
		\displaystyle \sum_{j=0}^{p}\{w_{0},...,\widehat{w_{j}},...,
		w_{p}\}=\displaystyle \sum_{i=0}^{p}\sum_{j=0}^{p}\{u_{0}, 
		w_{0},...,\widehat{u_{i}},\\\widehat{w_{j}},...,u_{p},w_{p}\}=
		B_{p}(\sigma+\tau)$. 

		Esto prueba que $B_{p}$ conmuta con la suma para 
		símplices. Se sigue que $\partial_{p}$ conmuta 
		con la suma para cadenas.

		Ahora debemos probar que $\partial_{p}$ conmuta con el 
		producto exterior, pero esto es inmediato pues:
		\begin{itemize}
			\item $\partial_{p}(0c)=\partial_{p}(0)=0=
				0\partial_{p}(c)$
			\item $\partial_{p}(1c)=\partial_{p}(c)=
				1\partial_{p}(c)$
		\end{itemize}
		Se concluye lo que queremos probar.
	\end{proof}

	Por lo tanto, ya tenemos casi construido nuestro complejo de cadenas 
	asociado a un complejo simplicial. Sólo resta enunciar el siguiente 
	resultado:

	\begin{theorem}
		Sea $\partial_{p}$ definida como en \ref{def:borde_form}. 
		Entonces para todo $p \in \{0,1,2,..\}$ $\partial_{p}\circ 
		\partial_{p+1}=0$. Coloquialmente, <<el borde del borde es 
		vacío>>.
	\end{theorem}
	\begin{proof}
		Sea $c \in C_{p+1}(\text{{\Large $\nu$}})$ y, sin pérdida
		de generalidad (en virtud de la definición 
		\ref{def:borde_form}), supongamos que $c=v,\text{ con }v\in 
		\text{{\Large $\nu$}}$. Es decir, $v$ es un elemento de la 
		base del espacio vectorial de cadenas $C_{p+1}(\text{
		{\Large $\nu$}})$.Veamos que $\partial_{p}(
		\partial_{p+1}(c))=0$.

		En efecto, notemos que, si $p\geq1$ (si $p=0$ es trivial), $v$ 
		posee $\binom{p+2}{p}$ 
		caras distintas de codimensión 2. Sea $\tau$ una de ellas, es 
		decir, $\tau$ es un $(p-1)$-símplice y $\tau \subset v$.

		Si probamos que $\tau$ aparece en 2 caras de codimensión 1 de 
		$v$ habremos terminado, pues aparecerá 2 veces al 
		hacer $\partial_{p}(\partial_{p+1}(c))$ y como estamos en 
		$\mathbb{Z}_{2}$ se anulará. Esto implica lo que queremos
		probar.

		Observemos que $\tau$ tiene cardinalidad $p$ mientras que $v$ 
		tiene dimensión $p+2$. Por lo tanto, supongamos, sin pérdida 
		de generalidad, que $\tau$ viene dado por los $p$ últimos 
		elementos de $v$. Así, tenemos dos elementos libres en $v$, y 
		al calcular las caras de codimensión 1 de $v$, con los $p$ 
		últimos elementos fijos, tendremos únicamente 2 caras que 
		contienen a $\tau$.  	
	\end{proof}

	\begin{remark}
		El teorema anterior denota lo significativo de la elección del
		anillo sobre el que se toman los coeficientes, pues la 
		demostración sería distinta y los cálculos posteriores se 
		complican. Veremos este hecho en los siguientes ejemplos.
	\end{remark}

	Veamos un ejemplo que ilustre la proposición anterior, es decir, que 
	<<el borde del borde es vacío>>.

	\begin{ejem}
		Supongamos el complejo simplicial {\Large $\nu$} del ejemplo 
		anterior y $\sigma = \{a,b,c\} \in \text{{\Large $\nu$}}$.

		Así pues, tendremos $c \in C_{2}(\text{{\Large $\nu$}})$, con
		$c=\begin{pmatrix}
			1 \\
			0 \\
			0 
		\end{pmatrix}$ la representación de $\sigma$ en $C_{2}(
		\text{{\Large $\nu$}})$. Ahora expresamos las aplicaciones 
		$\partial_{2}$ y $\partial_{1}$ en forma matricial:
		\begin{equation*}
			\partial_{2}= \begin{pmatrix}
				1 & 1 & 0 \\
				1 & 0 & 1 \\
				0 & 1 & 1 \\
				1 & 0 & 0 \\
				0 & 1 & 0 \\
				0 & 0 & 1
			\end{pmatrix}
			\hspace{0.5cm}\text{y}\hspace{0.5cm}
			\partial_{1}= \begin{pmatrix}
				1 & 1 & 1 & 0 & 0 &0 \\
				1 & 0 & 0 & 1 & 1 &0 \\
				0 & 1 & 0 & 1 & 0 &1 \\
				0 & 0 & 1 & 0 & 1 &1
			\end{pmatrix}
		\end{equation*}
		Ahora, teniendo en cuenta que estamos operando en un cuerpo de 
		característica 2, hacemos $\partial_{1}(\partial_{2}(c))$:
		\begin{equation*}
			\begin{split}
				\partial_{1}(\partial_{2}(c))=\begin{pmatrix}
				1 & 1 & 1 & 0 & 0 &0 \\
				1 & 0 & 0 & 1 & 1 &0 \\
				0 & 1 & 0 & 1 & 0 &1 \\
				0 & 0 & 1 & 0 & 1 &1
			\end{pmatrix}\cdot \Bigg( 
				\begin{pmatrix}
				1 & 1 & 0 \\
				1 & 0 & 1 \\
				0 & 1 & 1 \\
				1 & 0 & 0 \\
				0 & 1 & 0 \\
				0 & 0 & 1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
				1 \\
				0 \\
				0 
				\end{pmatrix} \Bigg ) = \\
				=\begin{pmatrix}
				1 & 1 & 1 & 0 & 0 &0 \\
				1 & 0 & 0 & 1 & 1 &0 \\
				0 & 1 & 0 & 1 & 0 &1 \\
				0 & 0 & 1 & 0 & 1 &1
				\end{pmatrix}
				\cdot
				\begin{pmatrix}
				1 \\
				1 \\
				0 \\
				1 \\
				0 \\
				0
				\end{pmatrix} =
				\begin{pmatrix}
                                0 \\
				0 \\
				0 \\
				0
				\end{pmatrix}=
				\vec{0}
			\end{split}
		\end{equation*}
		Hemos comprobado que, en efecto, <<el borde del borde>> de 
		$\sigma$ es 0. Para comprobarlo para cualquier vector bastará 
		observar que:
		\begin{equation*}
		\begin{pmatrix}
				1 & 1 & 1 & 0 & 0 &0 \\
				1 & 0 & 0 & 1 & 1 &0 \\
				0 & 1 & 0 & 1 & 0 &1 \\
				0 & 0 & 1 & 0 & 1 &1
			\end{pmatrix}\cdot 
				\begin{pmatrix}
				1 & 1 & 0 \\
				1 & 0 & 1 \\
				0 & 1 & 1 \\
				1 & 0 & 0 \\
				0 & 1 & 0 \\
				0 & 0 & 1
				\end{pmatrix}
				=
				\begin{pmatrix}
				0 & 0 & 0 \\
				0 & 0 & 0 \\
				0 & 0 & 0 \\
				0 & 0 & 0 
				\end{pmatrix}
		\end{equation*}
		En este ejemplo apreciamos lo significativo de elegir el 
		cuerpo $\mathbb{Z}_{2}$, pues en otro caso, los productos 
		matriciales son más difíciles de calcular.
	\end{ejem}

	Tras el resultado anterior y las construcciones previas, hemos acabado
	con la construcción del complejo de cadenas asociado a un complejo 
	simplicial. En consecuencia, ya estamos en posición de definir los
	\emph{espacios vectoriales de homología}.

	\begin{defi}
		\label{defi:homology}
		Sea $p \in \mathbb{N}\cup\{0\}$ y {\Large $\nu$} un complejo 
		simplicial. Definimos el \textit{p-ésimo espacio vectorial de 
		homología simplicial} del complejo simplicial {\Large $\nu$} 
		como el espacio vectorial cociente $Ker(\partial_{p})/
		Im(\partial_{p+1})$, donde $\partial_{p}$ está definida como
		en \ref{def:borde_form}. Lo denotaremos por $H_{p}(\text{
		{\Large $\nu$}})$.

		A su dimensión, $dimH_{p}(\text{{\Large $\nu$}})=
		dimKer(\partial_{p})-dimIm(\partial_{p+1})$, la 
		llamaremos \textit{p-ésimo número de Betti}, y la denotaremos
		por $\beta_{p}(\text{{\Large $\nu$}})$.
	\end{defi}

	Intuitivamente, los p-ciclos que no son p-bordes representan agujeros
	p-dimensionales. Por lo tanto, $\beta_{p}(\text{{\Large $\nu$}})$ 
	representa el número de p-agujeros de {\Large $\nu$}. Además, notemos 
	que si $dim\text{{\Large $\nu$}}=n$, entonces $\forall p >n 
	\hspace{0.25cm} H_{p}(\text{{\Large $\nu$}})=\emptyset$, pues {\Large 
	$\nu$}$_{p}=\emptyset$.

	Ahora veamos un ejemplo (propuesto en \cite{}) en el que calculamos 
	los números de Betti dado un complejo simplicial abstracto.

	\begin{ejem}
		Supongamos el siguiente complejo simplicial abstracto:
		\begin{equation*} 
			\text{{\Large $\nu$}}=\{\{a\},\{b\},\{c\},\{d\},\{e\},
			\{a,b\},\{a,c\},\{a,d\},\{b,c\},\{c,d\},\{a,b,c\}\}
		\end{equation*}
		Construimos la secuencia de grupos de cadenas asociados:
		\begin{equation*}
			 \left.
			\begin{array}{ccccccccc}
				 &  &  & \partial_{2} &  & 
				\partial_{1} &  & \partial_{0} & \\ 
				\emptyset & \longrightarrow & C_{2}(
				\text{{\Large $\nu$}}) & 
				\longrightarrow & C_{1}(\text{{\Large $\nu$}}) 
						& \longrightarrow 
				& C_{0}(\text{{\Large $\nu$}}) & 
				\longrightarrow & \emptyset 
			\end{array}
			\right. 
		\end{equation*}
		Calculamos $\partial_{2}$ y $\partial_{1}$, y los expresamos 
		de manera matricial:
		\begin{equation*}
			\partial_{2}=
			\begin{pmatrix}
			1 \\
			1 \\
			0 \\
			1 \\
			0
			\end{pmatrix}
			\hspace{1cm}
			\partial_{1}=
			\begin{pmatrix}
			1 & 1 & 1 & 0 & 0\\
			1 & 0 & 0 & 1 & 0\\
			0 & 1 & 0 & 1 & 1\\
			0 & 0 & 1 & 0 & 1\\
			0 & 0 & 0 & 0 & 0
			\end{pmatrix}
		\end{equation*}
		Ahora para calcular $\beta_{0}(\text{{\Large $\nu$}}) 
		\text{ y } \beta_{1}(\text{{\Large $\nu$}})$ bastará
		calcular el rango de las anteriores matrices. Observamos que:
		\begin{equation*}
			\partial_{1}=
			\begin{pmatrix}
			1 & 1 & 1 & 0 & 0\\
			\hlightr{1} & 0 & 0 & 1 & 0\\
			0 & \hlightr{1} & 0 & 1 & 1\\
			0 & 0 & \hlightr{1} & 0 & 1\\
			0 & 0 & 0 & 0 & 0
			\end{pmatrix}
		\end{equation*}
		Con lo que, $dimIm(\partial_{1})=3 \text{ y } 
		dimKer(\partial_{1})=2$, y por lo tanto, $\beta_{1}(\text{
		{\Large $\nu$}})=1$ y $\beta_{0}(\text{{\Large $\nu$}})=2$. La
		representación gráfica de {\Large $\nu$} en $\mathbb{R}^{2}$
		nos queda:
		\begin{figure}[H]
			\label{fig:homs}
			\centering
			\begin{tikzpicture}
				%Nodes
				\coordinate      (a) 	at 	(0,0);
				\coordinate      (b) 	at 	(2,0);
				\coordinate      (c) 	at 	(2,2);
				\coordinate      (d) 	at 	(0,2);
				\coordinate      (e) 	at 	(4,1);
				
				%Lines
				\draw[thick] (a) -- (c);
				\draw[thick] (a) -- (b);
				\draw[thick] (a) -- (d);
				\draw[thick] (b) -- (c);
				\draw[thick] (d) -- (c);


				\fill[black!20, draw=black, thick] (a) -- (c) -- (b) -- cycle;
				\fill[black!20, draw=black, thick] (a) circle (3pt) node[black, left] {a};
				\fill[black!20, draw=black, thick] (b) circle (3pt) node[black, right]  {b};
				\fill[black!20, draw=black, thick] (c) circle (3pt) node[black, right] {c};
				\fill[black!20, draw=black, thick] (d) circle (3pt) node[black, left] {d};
				\fill[black!20, draw=black, thick] (e) circle (3pt) node[black, right] {e};

			\end{tikzpicture}
			\caption{Representación geométrica del complejo 
			simplicial {\Large $\nu$}.}
		\end{figure}
		Tal y como ya hemos comentado, los números de Betti nos 
		cuentan los agujeros p-dimensionales. En este caso, si 
		observamos la representación anterior, vemos que tenemos un 
		agujero 1-dimensional y dos componentes conexas, que se 
		corresponde con los números de Betti que hemos calculado. 

		Notemos que, al igual que en el ejemplo anterior, si 
		trabajamos sobre otro cuerpo que no sea $\mathbb{Z}_{2}$, los 
		rangos de las matrices podrían ser distintos. En este caso, si
		trabajamos sobre $\mathbb{Q}$, el rango de $\partial_{1}$ es 
		4. Esto afecta a los números de Betti. 
	\end{ejem}
	
	Al igual que hicimos para los complejos simpliciales, veamos como 
	podemos definir morfismos, de manera más general, entre los objetos 
	que estamos manejando.
	
	Consideremos una aplicación entre complejos simpliciales, $f:\text{
	{\Large $\nu$}}\rightarrow\text{{\Large $\nu$}}'$. Aplicando el mismo 
	razonamiento que para la definición del homomorfismo borde, tenemos 
	que $f$ induce un homomorfismo entre espacios vectoriales de cadenas :
	\begin{equation}
		\label{hom:induc}
		\left.
		\begin{array}{lll}
			\overline{f_{p}}:C_{p}(\text{{\Large $\nu$}}) & 
				\rightarrow & C_{p}(\text{{\Large $\nu$}}')
				\\[3pt] 
			\multicolumn{1}{r}{c=\displaystyle \sum_{\mathclap{
			\sigma \in \text{{\Large $\nu$}$_{p}$}}}\sigma} & 
			\mapsto & 
			\overline{f_{p}}(c)=\displaystyle \sum_{\mathclap{
			f(\sigma) \in \text{{\Large $\nu$}$_{p}'$}}}f(\sigma)
		\end{array}
		\right. 
	\end{equation}
	Además, tal $f$ nos permite construir la secuencia:
	\begin{equation}
		\label{hom: gruphom}
		 \left.
		\begin{array}{ccccccccccccc}
			 & & & \partial_{p} &  & 
			\partial_{p-1} &  & \partial_{2}&  & \partial_{1}&  & 
			\partial_{0} & \\ 
			\emptyset & \rightarrow & C_{p}(
			\text{{\Large $\nu$}}) & \rightarrow & C_{p-1}(
			\text{{\Large $\nu$}}) & \rightarrow &\cdots & 
			\rightarrow & C_{1}(
			\text{{\Large $\nu$}})& \rightarrow & C_{0}(
			\text{{\Large $\nu$}})& \rightarrow & \emptyset\\[3pt]

			 & & \downarrow \overline{f_{p}}& & \downarrow 
			\overline{f_{p-1}}& 
			 & & & \downarrow \overline{f_{1}}
			 & & \downarrow \overline{f_{0}} & & \\ [3pt]
			\emptyset & \rightarrow & C_{p}(
			\text{{\Large $\nu$}}') & \rightarrow & C_{p-1}(
			\text{{\Large $\nu$}}') & \rightarrow &\cdots & 
			\rightarrow & C_{1}(
			\text{{\Large $\nu$}}')& \rightarrow & C_{0}(
			\text{{\Large $\nu$}}')& \rightarrow & \emptyset \\
			 & & & \partial_{p}' &  & 
			\partial_{p-1}' &  & \partial_{2}' &  & \partial_{1}'&
					& \partial_{0}' &
		\end{array}
		\right. 
	\end{equation}
	De esta secuencia observamos que: 
	\begin{equation*}
		\overline{f_{p-1}}\circ\partial_{p}=
		\partial_{p}'\circ\overline{f_{p}} 
	\end{equation*}
	En consecuencia, $\overline{f_{p}}$ induce un 
	homomorfismo entre espacios vectoriales de homología:	
		\begin{align*}
			\left.
			\begin{array}{l}
				f_{p}:H_{p}(\text{{\Large $\nu$}})\rightarrow 
				H_{p}(\text{{\Large $\nu$}}')\\[3pt] 
				\hspace{1.35cm} [c] \mapsto [\overline{f_{p}
				(c)}]
			\end{array}
			\right.
		\end{align*}
	Concluimos que, dada una aplicación $f$ entre complejos simpliciales, 
	siempre es posible asociarle una aplicación $f_{p}$ entre grupos de 
	homología.
	
	\begin{remark}
		Este propiedad es muy importante, de hecho, se conoce como 
		\emph{funtorialidad} y pertenece al ámbito de la teoría de 
		categorías que queda fuera del alcance del presente trabajo. 
	\end{remark}

	Si bien los espacios vectoriales de homología de un complejo 
	simplicial abstracto nos aportan mucha información acerca de sus 
	características topológicas, esta información tiene bastante margen de 
	mejora pues no nos dice nada de la variable <<tiempo>>. Pero, ¿Cómo 
	introducimos la noción de tiempo en un complejo simplicial abstracto? 
	Esta pregunta motiva la siguiente definición:

	\begin{defi}
		Sea {\Large $\nu$} un complejo simplicial abstracto finito. 
		Consideremos la secuencia $ \text{{\Large $\nu$}$_{1}$} 
		\subset \text{{\Large $\nu$}$_{2}$} \subset \cdots \subset
		\text{{\Large $\nu$}$_{k-1}$} \subset 
		\text{{\Large $\nu$}$_{k}$} = \text{{\Large $\nu$}}$ de 
		subcomplejos simpliciales cualesquiera de {\Large $\nu$}. A
		{\Large $\nu$} junto con su secuencia de subcomplejos 
		simpliciales encajados lo llamaremos \textit{complejo 
		simplicial filtrado}. 
	\end{defi}

	Esta noción nos habilita la variable <<tiempo>>, pues nos permite 
	preguntarnos en que momento de la secuencia aparecerá una cierta 
	característica topológica y cuanto <<tiempo>> sobrevivirá dicha 
	característica.

	Hay muchas maneras de construir la secuencia complejos simpliciales, 
	por ejemplo, empleando el \emph{complejo simplicial de Čech}. Su 
	construcción se realiza de la siguiente manera:

	Sea {\Large $\nu$} un complejo simplicial y $\mathcal{U}$ un 
	cubrimiento de {\Large $\nu$}. Los p-símplices del complejo simplicial
	de Čech vendrán dados por la intersección no vacía de p+1 conjuntos de
	$\mathcal{U}$.

	Lo interesante de este método es que si $\mathcal{U}$ verifica ciertas
	condiciones, el \emph{Teorema del nervio} garantiza que el complejo de
	Čech recupera la homología de {\Large $\nu$}. Ahora bien, ¿Cómo 
	podemos capturar y visualizar esta nueva información? Empleando la 
	\emph{homología persistente}.

	\begin{remark}
	En el presente trabajo nos ceñiremos a la homología persistente 
	definida sobre un complejo simplicial filtrado. No obstante, cabe 
	destacar que esta noción tiene una generalización conocida como 
	\emph{módulo de persistencia} que se define sobre un conjunto 
	parcialmente ordenado.
	\end{remark}

	\begin{defi}
	Sea $ \text{{\Large $\nu$}$_{1}$} 
		\subset \text{{\Large $\nu$}$_{2}$} \subset \cdots \subset
		\text{{\Large $\nu$}$_{k-1}$} \subset 
		\text{{\Large $\nu$}$_{k}$} = \text{{\Large $\nu$}}$ un 
		complejo simplicial filtrado. Definimos los \textit{p-ésimos
		espacios vectoriales de homología persistente} como las 
		imágenes de los homomorfismos inducidos por la inclusión, 
		$H_{p}^{i,j}=Imf_{p}^{i,j}$, con $0\leq i \leq j \leq k$.

		A su dimensión, $dimH_{p}^{i,j}$, la llamaremos \textit{p-ésimo 
		número de Betti persistente} y la denotaremos por 
		$\beta_{p}^{i,j}$.
	\end{defi}

	Los homomorfismos $f_{p}^{i,j}$ los definimos siguiendo la idea dada
	por la funtorialidad. Es decir, tendremos el diagrama:
	\begin{equation*}
		\resizebox{\textwidth}{!}{$ 
		\begin{array}{ccccccccccccc}
			 & & & \partial_{p}^{1} &  & 
			\partial_{p-1}^{1} &  & \partial_{2}^{1}&  & 
			\partial_{1}^{1}&  & 
			\partial_{0}^{1} & \\ 
			\emptyset & \rightarrow & C_{p}(
			\text{{\Large $\nu$}$_{1}$}) & \rightarrow & C_{p-1}(
			\text{{\Large $\nu$}$_{1}$}) & \rightarrow &\cdots & 
			\rightarrow & C_{1}(
			\text{{\Large $\nu$}$_{1}$})& \rightarrow & C_{0}(
			\text{{\Large $\nu$}$_{1}$})& \rightarrow & 
			\emptyset\\[3pt]

			& & \downarrow \overline{f_{p}^{1,2}}& \partial_{p}^{2} & 
			\downarrow \overline{f_{p-1}^{1,2}}& \partial_{p-1}^{2} 
			& & \partial_{2}^{2} & \downarrow \overline{f_{1}^{1,2}}
			& \partial_{1}^{2} & \downarrow \overline{f_{0}^{1,2}} & 
			\partial_{0}^{2} & \\ [3pt]	
			\emptyset & \rightarrow & C_{p}(
			\text{{\Large $\nu$}$_{2}$}) & \rightarrow & C_{p-1}(
			\text{{\Large $\nu$}$_{2}$}) & \rightarrow &\cdots & 
			\rightarrow & C_{1}(
			\text{{\Large $\nu$}$_{2}$})& \rightarrow & C_{0}(
			\text{{\Large $\nu$}$_{2}$})& \rightarrow & 
			\emptyset \\[5pt]
			
			\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & 
			\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & 
			\vdots \\[5pt]
			
			& & & \partial_{p}^{k-1} &  & 
			\partial_{p-1}^{k-1} &  & \partial_{2}^{k-1}&  & 
			\partial_{1}^{k-1}&  & 
			\partial_{0}^{k-1} & \\ 
			\emptyset & \rightarrow & C_{p}(
			\text{{\Large $\nu$}$_{k-1}$}) & \rightarrow & C_{p-1}(
			\text{{\Large $\nu$}$_{k-1}$}) & \rightarrow &\cdots & 
			\rightarrow & C_{1}(
			\text{{\Large $\nu$}$_{k-1}$})& \rightarrow & C_{0}(
			\text{{\Large $\nu$}$_{k-1}$})& \rightarrow & 
			\emptyset\\[3pt]

			& & \downarrow \overline{f_{p}^{k-1,k}}& \partial_{p}^{k} & 
			\downarrow \overline{f_{p-1}^{k-1,k}}& \partial_{p-1}^{k} 
			& & \partial_{2}^{k} & \downarrow \overline{f_{1}^{k-1,k}}
			& \partial_{1}^{k} & \downarrow \overline{f_{0}^{k-1,k}} & 
			\partial_{0}^{k} & \\ [3pt]	
			\emptyset & \rightarrow & C_{p}(
			\text{{\Large $\nu$}$_{k}$}) & \rightarrow & C_{p-1}(
			\text{{\Large $\nu$}$_{k}$}) & \rightarrow &\cdots & 
			\rightarrow & C_{1}(
			\text{{\Large $\nu$}$_{k}$})& \rightarrow & C_{0}(
			\text{{\Large $\nu$}$_{k}$})& \rightarrow & \emptyset \\
		\end{array}$}
	\end{equation*}
	Donde los homomorfismos $\overline{f_{p}^{i,j}}$ entre espacios 
	vectoriales de 
	cadenas vienen inducidos por el homomorfismo inclusión $f^{i,j}: 
	\text{{\Large $\nu$}$_{i}$} \xhookrightarrow{} 
	\text{{\Large $\nu$}$_{j}$}$ con $0\leq i \leq j \leq k$, como en 
	\ref{hom:induc}. Ahora, aplicando el mismo razonamiento que en 
	\ref{hom: gruphom} definimos los homomorfismos entre espacios 
	vectoriales de 
	homología $f^{i,j}_{p}$.

	Ahora que ya tenemos una herramienta que nos captura las 
	características topológicas junto con la variable <<tiempo>> en un 
	complejo simplicial abstracto filtrado, necesitamos una manera gŕafica 
	de visualizar esta información. Para ello, emplearemos los  
	\emph{diagramas de barras} y los \emph{diagramas de persistencia}.
	
	Para construir los diagramas de barras, dibujamos $\beta_{p}^{i,i+1}$ 
	puntos para la i-ésima filtración, y los conectamos con los 
	$\beta_{p}^{i+1,i+2}$ puntos de la filtración i+1 atendiendo al 
	siguiente crietrio: unimos los puntos $a$ de la filtración i y $b$ de
	la filtración i+1, si la clase del elemento que genera a $a$ es 
	preimagen por $f_{p}^{i,i+1}$ de la clase del elemento que genera a 
	$b$. Si la clase del elemento que genera a $a$ es enviada a 0 por 
	$f_{p}^{i,i+1}$ dibujaremos una linea que sale de $a$ en la filtración
	i hasta la filtración i+1. 

	En este caso, diremos que la clase del 
	elemento que genera a $a$ \emph{muere} en la filtración i+1. Si la 
	preimagen de la clase del elemento que genera a $a$ por 
	$f_{p}^{i-1,i}$ es el 0, diremos que la clase \emph{nace} en la 
	filtración i.

	Para los diagramas de persistencia, dibujaremos $\sum_{p}
	\beta_{p}^{i,j}$ puntos cuyas coordenadas en $\mathbb{R}^{2}$ vendrán 
	dadas por su filtración de nacimiento y de muerte en ese orden. 

	\begin{remark}
		Estos diagramas dependen de la elección de la base de los 
		espacios vectoriales subyacentes. Una mala elección nos 
		llevará a diagramas ilegibles. Por suerte, tenemos un 
		resultado teórico que nos garantiza la existencia de una 
		<<buena>> base.
	\end{remark}

	Veamos un ejemplo (propuesto en \cite{}) ilustrativo de estos 
	conceptos:

	\begin{ejem}
	Consideremos el siguiente complejo simplicial filtrado:  
	$ \text{{\Large $\nu$}$_{1}$} 
		\subset \text{{\Large $\nu$}$_{2}$} \subset
		\text{{\Large $\nu$}$_{3}$} \subset 
		\text{{\Large $\nu$}$_{4}$} = \text{{\Large $\nu$}}$. Donde:
	\begin{itemize}
		
		\item
			$\text{{\Large $\nu$}$_{1}$}=\{\{a\},\{b\},\{c\},\{d\}
			,\{a,b\}\}$
		\item
			$\text{{\Large $\nu$}$_{2}$}=\{\{a\},\{b\},\{c\},
				\{d\},\{e\},\{f\},\{a,b\},\{a,c\},\{b,c\}\}$
		\item 
			$\text{{\Large $\nu$}$_{3}$}=\{\{a\},\{b\},\{c\},
				\{d\}, \{e\},\{f\},\{a,b\},\{a,c\},\{a,e\},
				\{b,c\},\{b,f\},\{c,f\},\{c,e\},$\\
			$\{a,b,c\}\}$
		\item 
			$\text{{\Large $\nu$}$_{4}$}=\{\{a\},\{b\},\{c\},
				\{d\},\{e\},\{f\},\{a,b\},\{a,c\},\{a,e\},
				\{b,c\},\{b,f\},\{c,f\},\{c,e\},$\\
				$\{a,b,c\},\{a,c,e\}\}$
	\end{itemize}
	Veamos su representación gráfica en $\mathbb{R}^{2}$:
	\begin{figure}[H]
		\fbox{\minipage{0.225\textwidth}
			\begin{figure}[H]
				\resizebox{\textwidth}{!}{

			\begin{tikzpicture}
				%Nodes
				\node 		 (label) at (2,5) {{\Large $\nu$}$_{1}$};
				\coordinate      (a) 	at 	(0,0);
				\coordinate      (b) 	at 	(2,0);
				\coordinate      (c) 	at 	(2,2);
				\coordinate      (d) 	at 	(4,2);
				\coordinate      (e) 	at 	(2,4);
				\coordinate      (f) 	at 	(4,0);
				
				%Lines
				\draw[thick] (a) -- (b);


				\fill[black!20, draw=black, thick] (a) circle (3pt) node[black, below] {a};
				\fill[black!20, draw=black, thick] (b) circle (3pt) node[black, below]  {b};
				\fill[black!20, draw=black, thick] (c) circle (3pt) node[black, right] {c};
				\fill[black!20, draw=black, thick] (d) circle (3pt) node[black, right] {d};

		\end{tikzpicture}}
			\end{figure}	
		\endminipage}
		\fbox{\minipage{0.225\textwidth}
			\begin{figure}[H]
				\resizebox{1.0\textwidth}{!}{
				\begin{tikzpicture}
					%Nodes
					\node 		 (label) at (2,5) {{\Large $\nu$}$_{2}$};
					\coordinate      (a) 	at 	(0,0);
					\coordinate      (b) 	at 	(2,0);
					\coordinate      (c) 	at 	(2,2);
					\coordinate      (d) 	at 	(4,2);
					\coordinate      (e) 	at 	(2,4);
					\coordinate      (f) 	at 	(4,0);
					
					%Lines
					\draw[thick] (a) -- (c);
					\draw[thick] (a) -- (b);
					\draw[thick] (b) -- (c);

					\fill[black!20, draw=black, thick] (a) circle (3pt) node[black, below] {a};
					\fill[black!20, draw=black, thick] (b) circle (3pt) node[black, below]  {b};
					\fill[black!20, draw=black, thick] (c) circle (3pt) node[black, right] {c};
					\fill[black!20, draw=black, thick] (d) circle (3pt) node[black, right] {d};
					\fill[black!20, draw=black, thick] (e) circle (3pt) node[black, right] {e};
					\fill[black!20, draw=black, thick] (f) circle (3pt) node[black, right] {f};

				\end{tikzpicture}}
			\end{figure}	
		\endminipage}
		\fbox{\minipage{0.225\textwidth}
			\begin{figure}[H]
				\resizebox{1.0\textwidth}{!}{
				\begin{tikzpicture}
					%Nodes
					\node 		 (label) at (2,5) {{\Large $\nu$}$_{3}$};
					\coordinate      (a) 	at 	(0,0);
					\coordinate      (b) 	at 	(2,0);
					\coordinate      (c) 	at 	(2,2);
					\coordinate      (d) 	at 	(4,2);
					\coordinate      (e) 	at 	(2,4);
					\coordinate      (f) 	at 	(4,0);
					
					%Lines
					\draw[thick] (a) -- (e);
					\draw[thick] (c) -- (f);
					\draw[thick] (c) -- (e);
					\draw[thick] (b) -- (f);

					\fill[black!20, draw=black, thick] (a) -- (b) -- (c) -- cycle;
					\fill[black!20, draw=black, thick] (a) circle (3pt) node[black, below] {a};
					\fill[black!20, draw=black, thick] (b) circle (3pt) node[black, below]  {b};
					\fill[black!20, draw=black, thick] (c) circle (3pt) node[black, right] {c};
					\fill[black!20, draw=black, thick] (d) circle (3pt) node[black, right] {d};
					\fill[black!20, draw=black, thick] (e) circle (3pt) node[black, right] {e};
					\fill[black!20, draw=black, thick] (f) circle (3pt) node[black, right] {f};

				\end{tikzpicture}}
			\end{figure}	
		\endminipage}
		\fbox{\minipage{0.225\textwidth}
			\begin{figure}[H]
				\resizebox{1.0\textwidth}{!}{
				\begin{tikzpicture}
					%Nodes
					\node 		 (label) at (2,5) {{\Large $\nu$}$_{4}$};
					\coordinate      (a) 	at 	(0,0);
					\coordinate      (b) 	at 	(2,0);
					\coordinate      (c) 	at 	(2,2);
					\coordinate      (d) 	at 	(4,2);
					\coordinate      (e) 	at 	(2,4);
					\coordinate      (f) 	at 	(4,0);
					
					%Lines
					\draw[thick] (c) -- (f);
					\draw[thick] (b) -- (f);

					\fill[black!20, draw=black, thick] (a) -- (b) -- (c) -- cycle;
					\fill[black!20, draw=black, thick] (a) -- (e) -- (c) -- cycle;
					\fill[black!20, draw=black, thick] (a) circle (3pt) node[black, below] {a};
					\fill[black!20, draw=black, thick] (b) circle (3pt) node[black, below]  {b};
					\fill[black!20, draw=black, thick] (c) circle (3pt) node[black, right] {c};
					\fill[black!20, draw=black, thick] (d) circle (3pt) node[black, right] {d};
					\fill[black!20, draw=black, thick] (e) circle (3pt) node[black, right] {e};
					\fill[black!20, draw=black, thick] (f) circle (3pt) node[black, right] {f};

				\end{tikzpicture}}
			\end{figure}	
		\endminipage}
	\end{figure}
	Ahora vamos a dibujar los diagramas de barras correspondientes:
	\begin{figure}[H]
		\fbox{\minipage{0.48\textwidth}
			\begin{figure}[H]
				\resizebox{\textwidth}{!}{
				
				\begin{tikzpicture}[scale=2.5]
				%Nodes
				\node 		 (label) at (2,3) {Grado 0};
				\node 		 (label) at (2,-0.5) {Filtración};
				\coordinate      (n0) 	at 	(0,0);
				\coordinate      (n1) 	at 	(1,0);
				\coordinate      (n2) 	at 	(2,0);
				\coordinate      (n3) 	at 	(3,0);
				\coordinate      (n4) 	at 	(4,0);
				\coordinate      (n5) 	at 	(0,0.5);
				\coordinate      (n6) 	at 	(1,0.5);
				\coordinate      (n7) 	at 	(2,0.5);
				\coordinate      (n8) 	at 	(3,0.5);
				\coordinate      (n9) 	at 	(4,0.5);
				\coordinate      (n10) 	at 	(0,1);
				\coordinate      (n11) 	at 	(1,1);
				\coordinate      (n12) 	at 	(2,1);
				\coordinate      (n13) 	at 	(3,1);
				\coordinate      (n14) 	at 	(4,1);
				\coordinate      (n15) 	at 	(0,1.5);
				\coordinate      (n16) 	at 	(1,1.5);
				\coordinate      (n17) 	at 	(2,1.5);
				\coordinate      (n18) 	at 	(3,1.5);
				\coordinate      (n19) 	at 	(4,1.5);
				\coordinate      (n20) 	at 	(0,2);
				\coordinate      (n21) 	at 	(1,2);
				\coordinate      (n22) 	at 	(2,2);
				\coordinate      (n23) 	at 	(3,2);
				\coordinate      (n24) 	at 	(4,2);
				\coordinate      (n25) 	at 	(0,2.5);
				\coordinate      (n26) 	at 	(1,2.5);
				\coordinate      (n27) 	at 	(2,2.5);
				\coordinate      (n28) 	at 	(3,2.5);
				\coordinate      (n29) 	at 	(4,2.5);
				
				%Lines
				\draw[thick] (n0) -- (n3);
				\draw[thick,->] (n3) -- (n4);
				\draw[thick] (0,3pt) -- (0,-3pt) node[anchor=north] {1};
				\draw[thick] (1,3pt) -- (1,-3pt) node[anchor=north] {2};
				\draw[thick] (2,3pt) -- (2,-3pt) node[anchor=north] {3};
				\draw[thick] (3,3pt) -- (3,-3pt) node[anchor=north] {4};


				\draw[thick,->,color=blue] (n5) -- (n9);
				\draw[thick,color=blue] (n10) -- (n11);
				\draw[thick,->,color=blue] (n15) -- (n19);
				\draw[thick,color=blue] (n21) -- (n22);
				\draw[thick,color=blue] (n26) -- (n27);

		\end{tikzpicture}}
			\end{figure}	
		\endminipage}
		\fbox{\minipage{0.48\textwidth}
			\begin{figure}[H]
				\resizebox{\textwidth}{!}{
				\begin{tikzpicture}[scale=2.5]
					%Nodes
					\node 		 (label) at (2,3) {Grado 1};
					\node 		 (label) at (2,-0.5) {Filtración};
				\coordinate      (n0) 	at 	(0,0);
				\coordinate      (n1) 	at 	(1,0);
				\coordinate      (n2) 	at 	(2,0);
				\coordinate      (n3) 	at 	(3,0);
				\coordinate      (n4) 	at 	(4,0);
				\coordinate      (n5) 	at 	(0,0.5);
				\coordinate      (n6) 	at 	(1,0.5);
				\coordinate      (n7) 	at 	(2,0.5);
				\coordinate      (n8) 	at 	(3,0.5);
				\coordinate      (n9) 	at 	(4,0.5);
				\coordinate      (n10) 	at 	(0,1);
				\coordinate      (n11) 	at 	(1,1);
				\coordinate      (n12) 	at 	(2,1);
				\coordinate      (n13) 	at 	(3,1);
				\coordinate      (n14) 	at 	(4,1);
				\coordinate      (n15) 	at 	(0,1.5);
				\coordinate      (n16) 	at 	(1,1.5);
				\coordinate      (n17) 	at 	(2,1.5);
				\coordinate      (n18) 	at 	(3,1.5);
				\coordinate      (n19) 	at 	(4,1.5);
				\coordinate      (n20) 	at 	(0,2);
				\coordinate      (n21) 	at 	(1,2);
				\coordinate      (n22) 	at 	(2,2);
				\coordinate      (n23) 	at 	(3,2);
				\coordinate      (n24) 	at 	(4,2);
				\coordinate      (n25) 	at 	(0,2.5);
				\coordinate      (n26) 	at 	(1,2.5);
				\coordinate      (n27) 	at 	(2,2.5);
				\coordinate      (n28) 	at 	(3,2.5);
				\coordinate      (n29) 	at 	(4,2.5);
				
				%Lines
				\draw[thick] (n0) -- (n3);
				\draw[thick,->] (n3) -- (n4);
				\draw[thick] (0,3pt) -- (0,-3pt) node[anchor=north] {1};
				\draw[thick] (1,3pt) -- (1,-3pt) node[anchor=north] {2};
				\draw[thick] (2,3pt) -- (2,-3pt) node[anchor=north] {3};
				\draw[thick] (3,3pt) -- (3,-3pt) node[anchor=north] {4};


				\draw[thick,color=red] (n6) -- (n7);
				\draw[thick,->,color=red] (n12) -- (n14);
				\draw[thick,color=red] (n17) -- (n18);

				\end{tikzpicture}}
			\end{figure}	
		\endminipage}
	\end{figure}
	Por último, visualizamos los diagramas de persistencia:
	\begin{figure}[H]
		\fbox{\minipage{0.48\textwidth}
			\begin{figure}[H]
				\resizebox{\textwidth}{!}{
				
				\begin{tikzpicture}[scale=2.5]
				%Nodes
				\node 		 (label) at (2,4) {Grado 0};
				\node 		 (label) at (2,-0.5) {Nacimiento};
				\node[rotate=90] 		 (label) at (-0.5,1.5) {Muerte};
				\coordinate      (n0) 	at 	(0,0);
				\coordinate      (n1) 	at 	(1,0);
				\coordinate      (n2) 	at 	(2,0);
				\coordinate      (n3) 	at 	(3,0);
				\coordinate      (n4) 	at 	(4,0);
				\coordinate      (n5) 	at 	(0,0.5);
				\coordinate      (n6) 	at 	(1,0.5);
				\coordinate      (n7) 	at 	(2,0.5);
				\coordinate      (n8) 	at 	(3,0.5);
				\coordinate      (n9) 	at 	(4,0.5);
				\coordinate      (n10) 	at 	(0,1);
				\coordinate      (n11) 	at 	(1,1);
				\coordinate      (n12) 	at 	(2,1);
				\coordinate      (n13) 	at 	(3,1);
				\coordinate      (n14) 	at 	(4,1);
				\coordinate      (n15) 	at 	(0,1.5);
				\coordinate      (n16) 	at 	(1,1.5);
				\coordinate      (n17) 	at 	(2,1.5);
				\coordinate      (n18) 	at 	(3,1.5);
				\coordinate      (n19) 	at 	(4,1.5);
				\coordinate      (n20) 	at 	(0,2);
				\coordinate      (n21) 	at 	(1,2);
				\coordinate      (n22) 	at 	(2,2);
				\coordinate      (n23) 	at 	(3,2);
				\coordinate      (n24) 	at 	(4,2);
				\coordinate      (n25) 	at 	(0,2.5);
				\coordinate      (n26) 	at 	(1,2.5);
				\coordinate      (n27) 	at 	(2,2.5);
				\coordinate      (n28) 	at 	(3,2.5);
				\coordinate      (n29) 	at 	(4,2.5);
				
				%Lines
				\draw[thick] (n0) -- (n3);
				\draw[thick,->] (n3) -- (n4);
				\draw[thick] (0,2pt) -- (0,-2pt) node[anchor=north] {1};
				\draw[thick] (1,2pt) -- (1,-2pt) node[anchor=north] {2};
				\draw[thick] (2,2pt) -- (2,-2pt) node[anchor=north] {3};
				\draw[thick] (3,2pt) -- (3,-2pt) node[anchor=north] {4};
				
				\draw[thick,->] (n0) -- (0,3.7);
				\draw[thick] (2pt,1) -- (-2pt,1) node[anchor=north] {2};
				\draw[thick] (2pt,2) -- (-2pt,2) node[anchor=north] {3};
				\draw[thick] (2pt,3) -- (-2pt,3) node[anchor=north] {4};
				\node[anchor=east] at (-0.05,3.5) {$\infty$};


				\draw[thick,color=blue] (n0) -- (4,3.5);
				\draw[thick] (0,3.5) -- (4,3.5);
				\fill[blue] (0,1) circle (1pt);
				\fill[blue] (1,2) circle (2pt);
				\fill[blue] (0,3.5) circle (2pt);

		\end{tikzpicture}}
			\end{figure}	
		\endminipage}
		\fbox{\minipage{0.48\textwidth}
			\begin{figure}[H]
				\resizebox{\textwidth}{!}{
				
				\begin{tikzpicture}[scale=2.5]
				%Nodes
				\node 		 (label) at (2,4) {Grado 1};
				\node 		 (label) at (2,-0.5) {Nacimiento};
				\node[rotate=90] 		 (label) at (-0.5,1.5) {Muerte};
				\coordinate      (n0) 	at 	(0,0);
				\coordinate      (n1) 	at 	(1,0);
				\coordinate      (n2) 	at 	(2,0);
				\coordinate      (n3) 	at 	(3,0);
				\coordinate      (n4) 	at 	(4,0);
				\coordinate      (n5) 	at 	(0,0.5);
				\coordinate      (n6) 	at 	(1,0.5);
				\coordinate      (n7) 	at 	(2,0.5);
				\coordinate      (n8) 	at 	(3,0.5);
				\coordinate      (n9) 	at 	(4,0.5);
				\coordinate      (n10) 	at 	(0,1);
				\coordinate      (n11) 	at 	(1,1);
				\coordinate      (n12) 	at 	(2,1);
				\coordinate      (n13) 	at 	(3,1);
				\coordinate      (n14) 	at 	(4,1);
				\coordinate      (n15) 	at 	(0,1.5);
				\coordinate      (n16) 	at 	(1,1.5);
				\coordinate      (n17) 	at 	(2,1.5);
				\coordinate      (n18) 	at 	(3,1.5);
				\coordinate      (n19) 	at 	(4,1.5);
				\coordinate      (n20) 	at 	(0,2);
				\coordinate      (n21) 	at 	(1,2);
				\coordinate      (n22) 	at 	(2,2);
				\coordinate      (n23) 	at 	(3,2);
				\coordinate      (n24) 	at 	(4,2);
				\coordinate      (n25) 	at 	(0,2.5);
				\coordinate      (n26) 	at 	(1,2.5);
				\coordinate      (n27) 	at 	(2,2.5);
				\coordinate      (n28) 	at 	(3,2.5);
				\coordinate      (n29) 	at 	(4,2.5);
				
				%Lines
				\draw[thick] (n0) -- (n3);
				\draw[thick,->] (n3) -- (n4);
				\draw[thick] (0,2pt) -- (0,-2pt) node[anchor=north] {1};
				\draw[thick] (1,2pt) -- (1,-2pt) node[anchor=north] {2};
				\draw[thick] (2,2pt) -- (2,-2pt) node[anchor=north] {3};
				\draw[thick] (3,2pt) -- (3,-2pt) node[anchor=north] {4};
				
				\draw[thick,->] (n0) -- (0,3.7);
				\draw[thick] (2pt,1) -- (-2pt,1) node[anchor=north] {2};
				\draw[thick] (2pt,2) -- (-2pt,2) node[anchor=north] {3};
				\draw[thick] (2pt,3) -- (-2pt,3) node[anchor=north] {4};
				\node[anchor=east] at (0,3.5) {$\infty$};


				\draw[thick,color=red] (n0) -- (4,3.5);
				\draw[thick] (0,3.5) -- (4,3.5);
				\fill[red] (2,3) circle (1pt);
				\fill[red] (1,2) circle (1pt);
				\fill[red] (2,3.5) circle (1pt);

		\end{tikzpicture}}
			\end{figure}	
		\endminipage}
	\end{figure}
	\end{ejem}

	\newpage

	\subsection{Redes neuronales. Grafos subyacentes}
	
	Ahora cambiamos completamente de tema: nos alejamos momentáneamente de 
	la formal topología algebraica y nos acercamos al mundo 
	de la informática. En esta sección haremos una
	introducción hacia el otro concepto fundamental del presente trabajo: 
	\emph{las redes neuronales artificiales}. A lo largo de esta sección
	seguiremos la guía proporcionada por \cite{}.

	La historia del \emph{aprendizaje profundo} y de las redes neuronales 
	es muy amplia y queda fuera del alcance del presente trabajo. No 
	obstante, conviene reseñar que, aunque los términos aprendizaje 
	profundo y red neuronal nos parezcan algo reciente, la realidad es muy
	distinta. El origen de esta disciplina data de los años 40 de la mano
	de McCulloch y Pitts, y ha sufrido varias etapas de desarrollo hasta 
	llegar a nuestros días. Al igual que con otras líneas de 
	investigación, la inteligencia artificial (y en particular el 
	aprendizaje profundo) sufrió varios estancamientos y contratiempos 
	hasta llegar a nuestros días (<<inviernos de la IA>>). Por lo tanto, 
	debemos agradecer a todos aquellos investigadores que, aun cuando no 
	existía certeza del éxito de la inteligencia artificial, continuaron 
	con su trabajo en la disciplina y la desarrollaron hasta nuestros 
	días.

	Supongamos que queremos que nuestro ordenador nos escriba con palabras
	un número entre 0 y 9, dado dicho número en cifras. Parece una tarea 
	sencilla, ¿no?
	Bastará con desarrollar un programa que a partir de una entrada (la 
	cifra) nos produzca una salida (la palabra) de entre las nueve 
	posibilidades; en resumidas cuentas, con unas pocas sentencias 
	condicionales tendremos el resultado esperado.  

	Pensemos ahora en la 
	misma tarea, pero esta vez le proporcionaremos como entrada una imagen
	de la cifra manuscrita. El problema se acaba de volver 
	significativamente más difícil. La aproximación que propone el 
	aprendizaje automático es dejar que el ordenador <<aprenda>> a hacer 
	esa tarea. La idea intuitiva es desarrollar un programa que tome dos 
	entradas: la imagen y unos parámetros; y que a partir de estas 
	entradas proporcione una salida, tras esto, ajustar automáticamente 
	los parámetros para mejorar el desempeño del programa. Iterando este 
	proceso de predicción-ajuste, llegaremos a proporcionar la salida 
	correcta. Una vez que hemos encontrado los parámetros correctos,
	podemos abandonar este proceso de ajuste y tendremos un 
	programa clásico que a partir de unas entradas produce unas salidas.

	Teniendo clara la idea intuitiva, vamos a ver su desarrollo hasta 
	llegar a las redes neuronales.

	Supongamos que queremos modelar la relación lineal entre una variable
	objetivo ($y$) y $p$ variables independientes. Este modelo se conoce 
	como regresión lineal y puede ser expresado como:
	\begin{equation*}
		\hat{y}=w_{1}x_{1}+w_{2}x_{2}+\cdots+w_{p}x_{p}+b
	\end{equation*}
	donde $\hat{y}$ es valor estimado de $y$, $w_{1},w_{2},\cdots,w_{p}$ 
	son los pesos que indican la importancia de cada variable 
	independiente para el modelo, y $b$ es el término independiente. En 
	principio, $w_{1},w_{2},\cdots,w_{p}$ y $b$ toman valores en 
	$\mathbb{R}$.
	Siguiendo la terminología anterior, $\hat{y}$ es nuestra salida, las 
	variables independientes son nuestras entradas y $w_{1},w_{2},\cdots,
	w_{p}$ son los parámetros a ajustar. 

	Para realizar este ajuste será necesario tener una métrica que nos 
	indique si un conjunto de parámetros es mejor que otro. Para ello,
	usaremos el error cuadrático medio cuya expresión es la siguiente:

	\begin{equation*}
		J(w_{1},w_{2},\cdots,w_{p},b)=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^2
	\end{equation*}

	donde $n$ es el número de datos (cada dato es un vector de $p$ 
	entradas), $y_{i}$ es la salida real y 
	$\hat{y}_{i}$ es la salida estimada por el modelo. Nuestro objetivo
	es el de minimizar $J(w_{1},w_{2},\cdots,w_{p},b)$ para que el modelo 
	sea el mejor posible. 
	Para minimizar esta función será necesario ir ajustando los parámetros 
	correctamente. Para ello se emplea el algoritmo conocido como 
	\emph{el descenso del gradiente}.

	Supongamos una función continua y suave. Por ser ella continua sabemos
	que mapea puntos <<cercanos>> a puntos <<cercanos>>, y por ser suave,
	sabemos que podemos aproximar valores <<cercanos>> a un punto de la 
	función por una función lineal cuya pendiente sea la diferencial de la 
	función. Este resultado se conoce como el \emph{teorema de Taylor}. 
	Como bien sabemos, el gradiente de una función en un punto 
	nos indica cuanto aumenta o disminuye el valor 
	de dicha función ante un pequeño cambio en su entrada. Lo que haremos 
	será <<movernos un poco>> en la dirección marcada por el gradiente 
	pero en sentido opuesto, pues nuestro objetivo 
	es el de minimizar la función. Iterando este proceso,
	iremos dando pequeños <<pasos>> minimizando $J(w_{1},w_{2},\cdots,
	w_{p},b)$. En concreto, la 
	actualización de los parámetros es como sigue:
	
	\begin{equation*}
	\begin{array}{c}
		w_{i}^{siguiente}=w_{i}^{actual}-\alpha\frac{\partial J(w,b)}
		{\partial w_{i}}\\
		b^{siguiente}=b^{actual}-\alpha\frac{\partial J(w,b)}
		{\partial w_{i}}
	\end{array}
	\end{equation*}
	donde $\alpha$ es el ratio de aprendizaje: un hiperparámetro que 
	tenemos que elegir. Este paso de actualización se conoce como 
	\emph{propagación hacia atrás}, y el paso del cálculo de $\hat{y}$ se 
	conoce como \emph{propagación hacia adelante}. Una vez hemos logrado 
	la 
	configuración óptima de los parámetros ya podemos emplear este modelo 
	para la predicción de $y$.
	
	\begin{remark}
	Notemos que el proceso presentado logra teóricamente la convergencia 
	al mínimo local de $J(w_{1},w_{2},\cdots,w_{p},b)$. Para converger de 
	manera teórica al 
	mínimo global son necesarias técnicas más avanzadas cuya descripción 
	queda fuera del alcance del presente trabajo. Para más información 
	véase \cite{}.
	\end{remark}

	Desafortunadamente, el modelo de regresión lineal es muy limitado: 
	supone una relación lineal entre la variable objetivo y las variables
	independientes. Se hace necesaria una generalización que conseguriemos
	introduciendo una función no lineal a la regresión lineal. El primer
	modelo conocido con esta definición es el \emph{perceptrón}. Este 
	modelo se expresa de la siguiente manera:
	\begin{equation*}
		\hat{y}=r(w_{1}x_{1}+w_{2}x_{2}+\cdots+w_{p}x_{p}+b)
	\end{equation*}
	donde, en general, $r(x)$ es una función no lineal que se conoce como 
	\emph{función de activación} y el resto de variables son como en la 
	regresión lineal. En el caso particular del preceptrón, $r(x)$ está 
	definida de la siguiente manera:
	
	\begin{equation}
		\label{def:r}
		r(x)=\left \{
			\begin{array}{ll}
				1&\text{si }x>0\\
				0&\text{si }x\leq0
			\end{array} 
		     \right .
	\end{equation}

	\begin{remark}
	En el caso general, la función $r(x)$ es un hiperparámetro, es decir,
	debemos escoger una función no lineal. Lo habitual suele ser escoger
	la función sigmoide, la tangente hiperbólica o la función ReLU.
	\end{remark}
	
	Notemos que la función $r(x)$ es continua pero no es suave, pues ella
	no es diferenciable en $x=0$. Por lo tanto, teóricamente no podemos 
	asegurar que $J(w_{1},w_{2},\cdots,w_{p},b)$ lo sea. Sin embargo, por 
	cuestiones prácticas, 
	este incidente no se tiene en cuenta ya que normalmente no se alcanza
	el mínimo local de $J(w_{1},w_{2},\cdots,w_{p},b)$ luego no hay 
	problemas en que dicha 
	función no sea diferenciable en dicho punto. Adicionalmente, cabe 
	destacar que la mayoría de los algoritmos empleados para el cálculo de 
	la derivada en el descenso del gradiente sólo calculan una de las 
	derivadas laterales, luego el problema de la no diferenciabilidad 
	queda <<resuelto>>.

	Tras salvar el anterior problema, podemos razonar de manera análoga
	a como hemos hecho para la regresión lineal y tendremos el mismo 
	mecanismo para actualizar los parámetros y minimizar $J(w_{1},w_{2},
	\cdots,w_{p},b)$. 
	Obtenemos así un modelo más general que no presupone una relación 
	lineal entre la variable objetivo y las variables independientes. 
	
	A fin de facilitar el entendimiento de nuestros siguientes pasos, 
	vamos a representar gráficamente la estructura subyacente del 
	perceptrón.
	\begin{figure}[H]
		\centering
		\begin{tikzpicture}[
			roundnode/.style={circle, draw=black, thick, fill=white, minimum size=7mm},
			]
			%Nodes
			\node[roundnode]      (x1) 	at 	(-1,2)           {$x_{1}$};
			\node[roundnode]      (x2)      at 	(-1,1)		{$x_{2}$};
			\node      (p)      at 	(-1,0)		{$\vdots$};
			\node[roundnode]      (xn)      at 	(-1,-1)		{$x_{p}$};
			\node[roundnode]      (b)      at 	(-1,-2)		{$1$};
			\node[roundnode, text width=2.2cm, align=center]      (ac)      at 	(3,0)		{Función de activación $r$};
			\node[roundnode]      (dest)      at 	(11,0) {$\hat{y}$};
			
			\begin{pgfonlayer}{background}
			%Lines
			\draw[thick,->,rounded corners] (x1.mid) -| node[above,pos=0.2] {$w_{1}$} (1,0) -- (ac.west);
			\draw[thick,->,rounded corners] (x2.mid) -| node[above,pos=0.2] {$w_{2}$} (1,0) --(ac.west);
			\draw[thick,->,rounded corners] (xn.mid) -| node[above,pos=0.2] {$w_{p}$} (1,0) --(ac.west);
			\draw[thick,->,rounded corners] (b.mid) -| node[above,pos=0.2] {$b$} (1,0) --(ac.west);
			\draw[thick,->] (ac.east) -- node[above,align=left] {$r(w_{1}x_{1}+w_{2}x_{2}+\cdots+w_{p}x_{p}+b)$} (dest.west);
			
			\end{pgfonlayer}
		\end{tikzpicture}
		\caption{Representación gráfica de un perceptrón.}
	\end{figure}

	Esta representación gráfica y las limitaciones del perceptrón (para 
	más detalles véase \cite{}), motivan el siguiente paso en la 
	construcción de las \emph{redes neuronales}. Este paso consiste en 
	añadir más perceptrones en paralelo (nodos) y en serie (capas), 
	consiguiendo lo que se conoce como \emph{perceptrón multicapa}.

	El perceptrón multicapa es caso particular de arquitectura de una red 
	neuronal. En esta arquitectura las conexiones entre nodos (neuronas) 
	se realizan únicamente hacia adelante y no se permite la conexión 
	entre nodos de la misma capa, y la función de activación es 
	r(x) definida en \ref{def:r}. Para los propósitos del presente 
	trabajo, la función de activación no es relevante; pero sí lo es la 
	dirección de las conexiones entre los nodos. Por lo tanto, nos 
	ceñiremos al caso particular de las redes neuronales cuyas conexiones
	se realizan hacia adelante y sin conexiones dentro de la misma capa, 
	estas redes neuronales se conocen como \emph{redes neuronales 
	prealimentadas}. 
	
	\begin{remark}
	Aunque puede parecer que nos 
	estamos restringiendo a un caso muy particular de las redes 
	neuronales, el \emph{teorema de aproximación universal} nos garantiza
	que una red de estas características puede aproximar cualquier función
	Borel-medible cuyo dominio tenga dimensión finita (para más 
	información véase \cite{}).
	\end{remark}

	En el presente trabajo nos interesa especialmente la representación 
	gráfica de las redes neuronales prealimentadas, es decir, los grafos 
	subyacentes. En este sentido, veamos las siguientes definiciones:

	\begin{defi}
		Un \textit{grafo} $G=(V,E)$ consiste en un conjunto finito V, 
		cuyos 
		elementos reciben el nombre de \textit{vértices}, y un 
		conjunto 
		E de pares de elementos de V, cuyos elementos se conocen como 
		\textit{aristas}. Si $\{u,v\}$ es una arista de $G$, se dice 
		que los vértices $u$ y $v$ son \textit{adyacentes} y 
		llamaremos a $u$ y a $v$ \textit{extremos} de la arista.
	\end{defi}

	Para el caso las redes neuronales prealimentadas, necesitamos un caso 
	particular de grafo:

	\begin{defi}
		Un \textit{grafo dirigido} es un grafo en el que se asigna un 
		orden a los extremos de cada arista. Las aristas dirigidas se
		denotan $\{u,v\}$, y nótese que $\{u,v\}\neq\{v,u\}$. En el 
		caso de una arista dirigida $\{u,v\}$, se dice que $u$ es el 
		\textit{origen} y al vértice $v$ se le llama \textit{término} 
		de la arista. Un grafo dirigido se dice \textit{acíclico} si 
		para cada vértice $u$, no existe una sucesión de aristas cuyo
		origen y término sea $u$.
	\end{defi}

	En resumen, vamos a estudiar los grafos dirigidos acíclicos 
	subyacentes a las redes neuronales prealimentadas. A fin de facilitar 
	la comprensión, veamos un ejemplo:

	\begin{ejem}
	 Supondremos una red neuronal prealimentada de 2 capas y 13 neuronas.	
		\begin{figure}[H]
			\centering
			\begin{tikzpicture}[
				roundnode/.style={circle, draw=black, thick, fill=white, minimum size=7mm},
				]
				%Nodes
				\node[roundnode]      (n0) 	at 	(6,1)           {0};
				\node[roundnode]      (n1) 	at 	(6,-1)          {1};
				\node[roundnode]      (n2) 	at 	(4,3)           {2};
				\node[roundnode]      (n3) 	at 	(4,1)           {3};
				\node[roundnode]      (n4)      at 	(4,-1)		{4};
				\node[roundnode]      (n5)      at 	(4,-3)		{5};
				\node[roundnode]      (n6)      at 	(2,3)		{6};
				\node[roundnode]      (n7) 	at 	(2,1)           {7};
				\node[roundnode]      (n8) 	at 	(2,-1)           {8};
				\node[roundnode]      (n9)      at 	(2,-3)		{9};
				\node[roundnode]      (n10)      at 	(0,2)		{10};
				\node[roundnode]      (n11) 	at 	(0,0)           {11};
				\node[roundnode]      (n12) 	at 	(0,-2)           {12};
				
				\begin{pgfonlayer}{background}
				%Lines
				\draw[thick,->] (n10.mid) -- (n6.west);
				\draw[thick,->] (n10.mid) -- (n7.west);
				\draw[thick,->] (n10.mid) -- (n8.west);
				\draw[thick,->] (n10.mid) -- (n9.west);
				\draw[thick,->] (n11.mid) -- (n6.west);
				\draw[thick,->] (n11.mid) -- (n7.west);
				\draw[thick,->] (n11.mid) -- (n8.west);
				\draw[thick,->] (n11.mid) -- (n9.west);
				\draw[thick,->] (n12.mid) -- (n6.west);
				\draw[thick,->] (n12.mid) -- (n7.west);
				\draw[thick,->] (n12.mid) -- (n8.west);
				\draw[thick,->] (n12.mid) -- (n9.west);
				
				\draw[thick,->] (n6.mid) -- (n2.west);
				\draw[thick,->] (n6.mid) -- (n3.west);
				\draw[thick,->] (n6.mid) -- (n4.west);
				\draw[thick,->] (n6.mid) -- (n5.west);
				\draw[thick,->] (n7.mid) -- (n2.west);
				\draw[thick,->] (n7.mid) -- (n3.west);
				\draw[thick,->] (n7.mid) -- (n4.west);
				\draw[thick,->] (n7.mid) -- (n5.west);
				\draw[thick,->] (n8.mid) -- (n2.west);
				\draw[thick,->] (n8.mid) -- (n3.west);
				\draw[thick,->] (n8.mid) -- (n4.west);
				\draw[thick,->] (n8.mid) -- (n5.west);
				\draw[thick,->] (n9.mid) -- (n2.west);
				\draw[thick,->] (n9.mid) -- (n3.west);
				\draw[thick,->] (n9.mid) -- (n4.west);
				\draw[thick,->] (n9.mid) -- (n5.west);

				\draw[thick,->] (n2.mid) -- (n0.west);
				\draw[thick,->] (n2.mid) -- (n1.west);
				\draw[thick,->] (n3.mid) -- (n0.west);
				\draw[thick,->] (n3.mid) -- (n1.west);
				\draw[thick,->] (n4.mid) -- (n0.west);
				\draw[thick,->] (n4.mid) -- (n1.west);
				\draw[thick,->] (n5.mid) -- (n0.west);
				\draw[thick,->] (n5.mid) -- (n1.west);
				\end{pgfonlayer}
			\end{tikzpicture}
			\caption{Grafo dirigido acíclico asociado a una red neuronal de 13 neuronas y 2 capas.}
		\end{figure}
		Hemos etiquetado las neuronas (vértices) en orden ascendente 
		desde la primera neurona de la capa de salida hasta la última
		de la capa de entrada. En las próximas representaciones 
		seguiremos usando esta notación.
	\end{ejem}
	
	Habiendo fijado los conceptos de red neuronal prealimentada y de su 
	grafo asociado, necesitamos dar una serie de pasos previos hasta la 
	aplicación de la homología persistente. El primer paso será construir 
	un complejo simplicial a partir del grafo subycente a la red.
\end{document} 

